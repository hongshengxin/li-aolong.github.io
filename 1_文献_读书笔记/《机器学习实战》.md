## k-近邻

- 优点：精度高、对异常值不敏感、无数据输入假定。

  缺点：计算复杂度高、空间复杂度高。

  适用数据范围：数值型和标称型。

- k-近邻算法的一般流程

  (1) 收集数据：可以使用任何方法。

  (2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。

  (3) 分析数据：可以使用任何方法。

  (4) 训练算法：此步骤不适用于k-近邻算法。

  (5) 测试算法：计算错误率。

  (6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输

  入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。

## 决策树

- 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特

  征数据。

  缺点：可能会产生过度匹配问题。

  适用数据类型：数值型和标称型。

- 决策树的一般流程

  (1) 收集数据：可以使用任何方法。

  (2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。

  (3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。

  (4) 训练算法：构造树的数据结构。

  (5) 测试算法：使用经验树计算错误率。

  (6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据

  的内在含义。

## 朴素贝叶斯

- 优点：在数据较少的情况下仍然有效，可以处理多类别问题。

  缺点：对于输入数据的准备方式较为敏感。

  适用数据类型：标称型数据。

- 朴素贝叶斯的一般过程

  (1) 收集数据：可以使用任何方法。本章使用RSS源。

  (2) 准备数据：需要数值型或者布尔型数据。

  (3) 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。

  (4) 训练算法：计算不同的独立特征的条件概率。

  (5) 测试算法：计算错误率。

  (6) 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴

  素贝叶斯分类器，不一定非要是文本。

- 没看完

## Logistic回归

- 优点：计算代价不高，易于理解和实现。

  缺点：容易欠拟合，分类精度可能不高。

  适用数据类型：数值型和标称型数据。

- Logistic回归的一般过程

  (1) 收集数据：采用任意方法收集数据。

  (2) 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据

  格式则最佳。

  (3) 分析数据：采用任意方法对数据进行分析。

  (4) 训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。

  (5) 测试算法：一旦训练步骤完成，分类将会很快。

  (6) 使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；

  接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于

  哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。

## 支持向量机

- 优点：泛化错误率低，计算开销不大，结果易解释。

  缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。

  适用数据类型：数值型和标称型数据。

- SVM的一般流程

  (1) 收集数据：可以使用任意方法。

  (2) 准备数据：需要数值型数据。

  (3) 分析数据：有助于可视化分隔超平面。

  (4) 训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。

  (5) 测试算法：十分简单的计算过程就可以实现。

  (6) 使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类

  分类器，对多类问题应用SVM需要对代码做一些修改。

- 代码没写完

## AdaBoost

- 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。

  缺点：对离群点敏感。

  适用数据类型：数值型和标称型数据。

- AdaBoost的一般流程

  (1) 收集数据：可以使用任意方法。

  (2) 准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可

  以处理任何数据类型。当然也可以使用任意分类器作为弱分类器，第2章到第6章中的

  任一分类器都可以充当弱分类器。作为弱分类器，简单分类器的效果更好。

  (3) 分析数据：可以使用任意方法。

  (4) 训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练

  弱分类器。

  (5) 测试算法：计算分类的错误率。

  (6) 使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想把它应用到多个类

  别的场合，那么就要像多类SVM中的做法一样对AdaBoost进行修改。