## 知识点

- Word2vVec -> ELMO -> GPT -> Bert
- 从Word Embedding到ELMO
  - **ELMO**根据当前上下文对Word Embedding动态调整
  - ELMO是基于特征融合的预训练方法
- **transformer**解决了普通word embedding 没有上下文的问题，但是解决这个问题，需要大量的标注信息样本。如何解决transformer的问题，就引入了elmo
- **bert**为什么更好呢？
  - 单向信息流的问题 ,只能看前面，不能看后面，其实预料里有后面的信息，只是训练语言模型任务特殊要求只能看后面的信息，这是最大的一个问题。
  - 其次是pretrain 和finetuning 几个句子不匹配
- 具体怎么做才能让这个模型：看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息呢？**XLNet**在模型方面的主要贡献其实是在这里。
  - XLNet的基本思想：在随机排列组合后的各种可能里，再选择一部分作为模型预训练的输入

## 预训练相关

- [从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
- 文献：Rethinking ImageNet Pre-training

## XLNet

- [XLNet:运行机制及和Bert的异同比较](<https://zhuanlan.zhihu.com/p/70257427>)
- [香侬读 | XLnet：比Bert更强大的预训练模型](<https://zhuanlan.zhihu.com/p/71759544>)

## Attention

- [深度学习中的注意力模型（2017版）](<https://zhuanlan.zhihu.com/p/37601161>)

## 基准

- GLUE
- SQuAD
- RACE