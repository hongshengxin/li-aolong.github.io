# 李英豪——文献阅读

> 第6篇文献：[《Language Model Based Grammatical Error Correction without Annotated Training Data》](https://aclweb.org/anthology/W18-0529)

## 基于语言模型的无注释训练数据的GEC

### 1 简介

- 自CoNLL-2014结束以来，对基于语言模型(LM)的GEC方法的研究基本停滞不前。
- 本文表明了仅使用最少的注释数据（~1000个句子），完全有可能建立一个简单的系统，且可以获得有竞争力的结果。
- 相比之下，SMT和其它是需要大量标记数据的方法无法实现这一点

### 2 方法

- GEC中语言建模背后的核心思想是，低概率序列比高概率序列更可能包含语法错误
- 基于LM概率的GEC的目标是，确定如何将低概率序列转换为高概率序列
- 本文方法基于：Dahlmeier and Ng (2012a)，包含五步：
  1. 计算输入句子的归一化对数概率
  2. 为每个句子构建一个混淆集合
  3. 在混淆集合中为替代候选的句子重新打分
  4. 应用单个最佳校正，将概率提高到阈值以上
  5. 重复步骤1-4

#### 2.1 序列概率

需要进行正正规化

#### 2.2 混淆集合

基于LM的GEC的定义特征之一是该方法不一定需要带注释的训练数据，本文关注以下错误类型：

- 错别词：用CyHunspell v1.2.1模块来生成对于非单词的候选词
- 词形：名词数量；动词时态；形容词形式。使用数据库[AGID](<http://wordlist.aspell.net/other/>)
- 冠词和介词

#### 2.3 迭代

一次迭代只能修改一个单词是因为相互影响

### 3 数据和资源

- 在[十亿字基线数据](https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark)上使用[KenLM](https://kheafield.com/papers/avenue/kenlm.pdf)训练了5-gram语言模型
- 使用CoNLL-2013， CoNLL-2014，FCE和JFLEG来进行开发和测试
- ![1563793310831](7月22日-工作总结-语音识别后处理.images/1563793310831.png)

### 微调

- 用来确定F0.5的概率阈值
- 在开发集上尝试了0-10%的阈值，最佳阈值：CoNLL-2013(2%)，FCE-dev(4%)，JFLEG-dev(5%)
- ![1563793334366](7月22日-工作总结-语音识别后处理.images/1563793334366.png)

### 结果和讨论

- 将句子首字母改为大写使得测试分提高，表明不同的测试集具有非常不同的误差类型分布，即使最简单的校正策略也会显着影响结果。
- 开源代码：<https://github.com/chrisjbryant/lmgec-lite>

### 结论

- 本文证明了在本文系统收到可纠正错误类型范围的限制下，使用最少注释数据的GEC简单语言模型方法仍然可以与依赖于大量注释训练数据的最新神经和机器翻译方法竞争