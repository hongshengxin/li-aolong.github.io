# 李英豪——NLP相关了解

## 处理流程

- 语料预处理
  - 语料清洗：包括人工去重，对齐，删除，标注，规则提取内容，正则匹配，实体提取等
  - 分词：中文语料需要分词
  - 词性标注：基于规则，基于统计
  - 去停用词：不是必须的
- 特征工程
  - 词袋模型：统计词频
  - 词向量：将文字转换成向量矩阵进行计算，主要包括跳字模型(Skip-Gram)和连续词袋模型(Continuous Bag of Words)。还有Word2Vec，Doc2Vec，WordRank，FastText等
- 特征选择
  - 文本特征一般都是词语，具有语义信息
- 模型训练
  - 有监督和无监督机器学习模型；深度学习模型等
- 评价指标
  - 错误率，精度，准确率，精确度，召回率，F1衡量
  - ROC曲线，AUC

## jieba

### 分词算法

1. 基于统计词典，构造有向无环图(DAG)
2. 基于DAG图，采用动态规划计算最大概率路径来进行分词
3. 对于新词，采用HMM模型进行切分

### 相关函数

- 分词
  - 精确分词：`jieba.cut(text, cut_all=False)`
  - 全模型：`jieba.cut(text, cut_all=True)`
  - 搜索引擎模式：`jieba.cut_for_search(text)`
  - 生成list：`jieba.lcut(text)`
  - 获取词性：`jieba.psseg.lcut(text)`
  - 自定义添加词和字典：`jieba.add_word('text')`或`jieba.load_userdict('user_dict.txt')`