# 基于Attention的Encoder-Decoder模型

## Encoder-Decoder弊端

- **信息的有损压缩。**虽然Decoder接收到了输入序列的信息，但它却不能掌握完全的信息。因为Encoder是对输入序列进行了有损压缩，这意味着在传递信息的过程中，对信息有一定的损失
  - 如果输入的句子越长，那么在压缩过程中，损失的信息量就越大，进而导致Decoder对于模型的预测结果就越不好
  - ![img](https://pic1.zhimg.com/80/v2-5b4510552a695e5b479f735ae5f370a8_hd.png)
  - 句子长度的增加，BLEU会先增加后减小。也就意味着，对于一般句子的长度来说（10-20词之间），模型的翻译效果是相当好的，但是当句子越来越长，它的结果只会变得越来越差
- **RNN的时间维度过大。**当句子序列很长时，这就意味着RNN的时间维度很深，导致在BPTT过程中出现梯度弥散的现象。
- **Context Vector的同质性。**Encoder将输入序列转化为Context Vector以后传递给Decoder，Decoder在进行每一个词进行翻译的时候，它所参考的输入序列的信息都是Context Vector这样一个常量。所以在使用相同的Context Vector翻译时，会包含一些干扰信息，使得结果不够准确。

## Attention模型

首先先定义以下变量：

![img](https://pic1.zhimg.com/80/v2-c5e2e784df15cbf2f4170a6c7fdbcf8c_hd.png)

在原来的Encoder-Decoder模型中，我们实际上在最大化联合概率分布：

![img](https://pic1.zhimg.com/80/v2-60697366c09700b58d5260ae160e5fd8_hd.png)

其中，每一个条件概率可以表示为：

![img](https://pic4.zhimg.com/80/v2-15c1d04431413390fc6e5abf9b612253_hd.png)

其中g是非线性函数，也就是RNN，在Attention模型中，我们的的每一个条件概率变为：

![img](https://pic2.zhimg.com/80/v2-ca8e2785133ebf468d77d2c8ab6fb509_hd.png)

其中第i个阶段状态是关于前一个阶段状态、前一阶段输出以及第i个Context Vector的函数。

![img](https://pic1.zhimg.com/80/v2-b4cc91508480c86c691c40d2acba2dc4_hd.png)

在新的模型中，在对第i个目标值进行预测时，概率函数开始利用整个X输入的信息，意味着对于Decoder端，在翻译每一个单词的时候，都会有独一无二的Context Vector与它对应，这就解决了Context Vector同质性的问题。

下图展示了在预测第t个阶段的输出y时的结构，通过对Encoder层状态的加权，从而掌握输入语句中的所有细节信息。

![img](https://pic2.zhimg.com/80/v2-3d45ea51a64c62840e891fb1aae72251_hd.png)

