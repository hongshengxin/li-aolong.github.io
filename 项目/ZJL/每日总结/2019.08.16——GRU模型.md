# GRU模型

## 1. 概念

GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种，和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。

GRU和LSTM在很多情况下实际表现上相差无几，GRU的优势在于它的实验效果与LSTM相似，但是更易于计算。

相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。

## 2 GRU的输入输出结构

GRU的输入输出结构与普通的RNN是一样的。

有一个当前的输入x^t^，和上一个节点传递下来的隐状态（hidden state），这个隐状态包含了之前节点的相关信息。结合x^t^和h^t-1^，GRU会得到当前隐藏节点的输出和传递给下一个节点的隐状态。

![img](https://pic2.zhimg.com/v2-49244046a83e30ef2383b94644bf0f31_b.jpg)

## 3 GRU的内部结构

首先，先通过上一个传输下来的状态h^t-1^和当前节点的输入x^t^来获取两个门控状态。其中r​是控制重置的门控（reset gate），z为控制更新的门控（update gate）。

![img](https://pic3.zhimg.com/v2-7fff5d817530dada1b279c7279d73b8a_b.jpg)

得到门控信号之后，首先使用重置门控来得到**“重置”**之后的数据 ![[公式]](https://www.zhihu.com/equation?tex=%7Bh%5E%7Bt-1%7D%7D%27+%3D+h%5E%7Bt-1%7D+%5Codot+r+) ，再将 ![[公式]](https://www.zhihu.com/equation?tex=%7Bh%5E%7Bt-1%7D%7D%27) 与输入 ![[公式]](https://www.zhihu.com/equation?tex=x%5Et+) 进行拼接，再通过一个tanh激活函数来将数据放缩到**-1~1**的范围内得到![[公式]](https://www.zhihu.com/equation?tex=h%27)。

![img](https://pic4.zhimg.com/v2-390781506bbebbef799f1a12acd7865b_b.jpg)

这里的 ![[公式]](https://www.zhihu.com/equation?tex=h%27+) 主要包含了当前输入的 ![[公式]](https://www.zhihu.com/equation?tex=x%5Et) 数据，有针对性地对 ![[公式]](https://www.zhihu.com/equation?tex=h%27) 添加到当前的隐藏状态，相当于记忆了当前时刻的状态，类似于LSTM的选择记忆阶段。

![img](https://pic1.zhimg.com/v2-8134a00c243153bfd9fd2bcbe0844e9c_b.jpg)

接下来是**”更新记忆“**阶段，在这个阶段，同时进行了遗忘和记忆两个步骤，使用了先前得到的更新门控 ![[公式]](https://www.zhihu.com/equation?tex=z) （update gate）。

**更新表达式**： ![[公式]](https://www.zhihu.com/equation?tex=h%5Et+%3D+z+%5Codot+h%5E%7Bt-1%7D+%2B+%281+-+z%29%5Codot+h%27) 

GRU的优点在于，**使用了同一个门控 ![[公式]](https://www.zhihu.com/equation?tex=z) 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）**。

![[公式]](https://www.zhihu.com/equation?tex=z+%5Codot+h%5E%7Bt-1%7D) ：表示对原本隐藏状态的选择性“遗忘”。这里的 ![[公式]](https://www.zhihu.com/equation?tex=z) 类似于遗忘门（forget gate），忘记 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 维度中一些不重要的信息。

![[公式]](https://www.zhihu.com/equation?tex=%281-z%29+%5Codot+h%27) ： 表示对包含当前节点信息的 ![[公式]](https://www.zhihu.com/equation?tex=h%27) 进行选择性”记忆“。与上面类似，这里的 ![[公式]](https://www.zhihu.com/equation?tex=%281-z%29) 同理会忘记 ![[公式]](https://www.zhihu.com/equation?tex=h+%27) 维度中的一些不重要的信息。或者，这里我们更应当看做是对 ![[公式]](https://www.zhihu.com/equation?tex=h%27+) 维度中的某些信息进行选择。

![[公式]](https://www.zhihu.com/equation?tex=h%5Et+%3D+z+%5Codot+h%5E%7Bt-1%7D+%2B+%281+-+z%29%5Codot+h%27) ：结合上述，这一步的操作就是忘记传递下来的 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D+) 中的某些维度信息，并加入当前节点输入的某些维度信息。

可以看到，这里的遗忘 ![[公式]](https://www.zhihu.com/equation?tex=z) 和选择 ![[公式]](https://www.zhihu.com/equation?tex=%281-z%29) 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （![[公式]](https://www.zhihu.com/equation?tex=z) ），我们就会使用包含当前输入的 ![[公式]](https://www.zhihu.com/equation?tex=h%27) 中所对应的权重进行弥补 ![[公式]](https://www.zhihu.com/equation?tex=%281-z%29) 。以保持一种”恒定“状态。

## 4 LSTM与GRU的关系

- GRU是在2014年提出来的，而LSTM是1997年。
- ![[公式]](https://www.zhihu.com/equation?tex=r) (reset gate)仅仅被用来获得 ![[公式]](https://www.zhihu.com/equation?tex=h%E2%80%99) 。
- ![[公式]](https://www.zhihu.com/equation?tex=h%27)实际上可以看成对应于LSTM中的hidden state，上一个节点传下来的 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 则对应于LSTM中的cell state。
- ![[公式]](https://www.zhihu.com/equation?tex=z) 对应的则是LSTM中的forget gate，那么 ![[公式]](https://www.zhihu.com/equation?tex=%281-z%29) 就可以看成是选择门 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) 了

## 4. 总结

- GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。
- 与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但也能够达到与LSTM相当的功能
- 考虑到硬件的**计算能力**和**时间成本**，因而很多时候会选择更加”实用“的GRU

